#clean record
clean_rawData_helpful<- na.omit(clean_rawData_helpful)
clean_rawData_helpful[] <- lapply(clean_rawData_helpful, factor)
clean_rawData_helpful
# helpfull the plot
helpful_chart <- likert(clean_rawData_helpful)
plot(helpful_chart)
df.v<- summary(helpful_chart) #for references in result
Techdata <- clean_rawData %>%
select(`Amazon Machine Learning`:Tableau)
Techdata<- na.omit(Techdata)
Techdata
Techdata[ ,1:18]  <- lapply(Techdata[ ,1:18],
FUN = function(x) Recode(x, "'Nice to have' =1 ;'Necessary'=2 ;'Unnecessary' = 0"))
Techdata
temp1 <- names(Techdata)
temp2<- list(colSums(Techdata))
temp2<- data.frame(matrix(unlist(colSums(Techdata)), byrow=T))
scale <- cbind(temp1,temp2)
names(scale)[1] <-"Tech_Skill"
names(scale)[2] <-"Rank"
scale <- scale %>%
arrange(desc(Rank))
scale
#plot graph
scale %>%
mutate(Tech_Skill = fct_reorder(Tech_Skill, Rank)) %>%
ggplot( aes(x=Tech_Skill, y=Rank)) +
geom_segment( aes(xend=Tech_Skill, yend=1)) +
geom_point( size=2, color="orange") +
coord_flip() + labs(title="Technical Skills Survey")+
theme_minimal()
softdata <- clean_rawData %>%
select(`Intellectual curiosity`:`Generating Hypotheses`)
softdata<- na.omit(softdata)
softdata
temp3 <- names(softdata)
temp4<- data.frame(matrix(unlist(colSums(softdata)), byrow=T))
scale1 <- cbind(temp3,temp4)
names(scale1)[1] <-"Soft_skill"
names(scale1)[2] <-"Total"
scale1 <- scale1 %>%
arrange(desc(Total))
scale1
#plot graph
scale1 %>%
mutate(Soft_skill = fct_reorder(Soft_skill, Total)) %>%
ggplot( aes(x=Soft_skill, y=Total)) +
geom_segment( aes(xend=Soft_skill, yend=1)) +
geom_point( size=2, color="orange") +
coord_flip() + labs(title="Soft Skills Survey")+
theme_minimal()
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, out.width = "90%")
if (!require('ggplot2')) install.packages('ggplot2')
if (!require('car')) install.packages('car')
if (!require('rlist')) install.packages('rlist')
if (!require('tidyverse')) install.packages('tidyverse')
if (!require('likert')) install.packages('likert')
# Import survey data
rawData <- read.csv("https://raw.githubusercontent.com/Vinayak234/SPS_DATA_607/master/SPS_DATA_607/Project_3/To%20find%20Most%20Valued%20Data%20Science%20Skills.csv", stringsAsFactors = FALSE, header = TRUE, na.strings = c("", "NA"))
rawData
# Dropping none IT professionals records
clean_rawData<- rawData[!grepl("No", rawData$Are.you.related.to.IT.Profession.),]
#Drop column not needed
drops_c <- c("Timestamp","Are.you.related.to.IT.Profession.")
clean_rawData<-clean_rawData[ , !(names(clean_rawData) %in% drops_c)]
#Short name for column name
Short_name <- c("Gender","Country","Age","Are you in School","Eduction","Major","Are you learning data science skill","Emplyment Status", "Title", "Experience", "Blogs", "College","Projects", "Online Course", "Friends", "Co-workers", "Youtube","Textbooks","First language","must language","Amazon Machine Learning","Big Data","College Degree","Data Visualizations","Enterprise Tools","Google Cloud","Hadoop/Hive/Pig","IBM SPSS","Java","Microsoft Excel","NoSQL","Oracle Data Mining","Python","R","Relational data","SAS","SQL","Tableau","Intellectual curiosity","Business acumen", "Communication skills", "Teamwork", "Collaboration", "Creative Thinking","Problem Solving", "Active Learning","Perceptiveness","Interpersonal Skills","Generating Hypotheses")
colnames(clean_rawData) <- Short_name
head(clean_rawData)
temp1 <- names(Techdata)
temp2<- list(colSums(Techdata))
temp2<- data.frame(matrix(unlist(colSums(Techdata)), byrow=T))
scale <- cbind(temp1,temp2)
names(scale)[1] <-"Tech_Skill"
names(scale)[2] <-"Rank"
scale <- scale %>%
arrange(desc(Rank))
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, out.width = "90%")
if (!require('ggplot2')) install.packages('ggplot2')
if (!require('car')) install.packages('car')
if (!require('rlist')) install.packages('rlist')
if (!require('tidyverse')) install.packages('tidyverse')
if (!require('likert')) install.packages('likert')
# Import survey data
rawData <- read.csv("https://raw.githubusercontent.com/Vinayak234/SPS_DATA_607/master/SPS_DATA_607/Project_3/To%20find%20Most%20Valued%20Data%20Science%20Skills.csv", stringsAsFactors = FALSE, header = TRUE, na.strings = c("", "NA"))
rawData
# Dropping none IT professionals records
clean_rawData<- rawData[!grepl("No", rawData$Are.you.related.to.IT.Profession.),]
#Drop column not needed
drops_c <- c("Timestamp","Are.you.related.to.IT.Profession.")
clean_rawData<-clean_rawData[ , !(names(clean_rawData) %in% drops_c)]
#Short name for column name
Short_name <- c("Gender","Country","Age","Are you in School","Eduction","Major","Are you learning data science skill","Emplyment Status", "Title", "Experience", "Blogs", "College","Projects", "Online Course", "Friends", "Co-workers", "Youtube","Textbooks","First language","must language","Amazon Machine Learning","Big Data","College Degree","Data Visualizations","Enterprise Tools","Google Cloud","Hadoop/Hive/Pig","IBM SPSS","Java","Microsoft Excel","NoSQL","Oracle Data Mining","Python","R","Relational data","SAS","SQL","Tableau","Intellectual curiosity","Business acumen", "Communication skills", "Teamwork", "Collaboration", "Creative Thinking","Problem Solving", "Active Learning","Perceptiveness","Interpersonal Skills","Generating Hypotheses")
colnames(clean_rawData) <- Short_name
head(clean_rawData)
DataAge <- clean_rawData%>%
select(Age) %>%
filter(Age!='NA') %>%
group_by(Age) %>%
summarise(count=n()) %>%
arrange(desc(count))
head(DataAge, n=5)
Gender_ratio <- clean_rawData%>%
select(Gender)%>%
group_by(Gender)%>%
summarise(count=n())%>%
mutate(percent = (count / sum(count)) * 100) %>%
arrange(desc(count))
Gender_ratio
#to see how many of the participants were male or female by their age for the graph
DataGender <- clean_rawData%>%
select(Age,Gender) %>%
filter(trimws(clean_rawData$Country)!='',Age!='NA') %>%
group_by(Age,Gender) %>%
summarise(count=n()) %>%
arrange(desc(count))
# Plot
ggplot(data = DataGender,
mapping = aes(x = Age, fill = Gender,
y = ifelse(test = Gender == "Female",
yes = -count, no = count)))+
geom_bar(stat = "identity") +
scale_y_continuous(labels = abs) +
labs(y="Count")
DemoGraphicsData <- clean_rawData %>%
group_by(Country)%>% # Group by country
summarise(count=n()) %>%# Count how many respondents selected each option
mutate(percent = (count / sum(count)) * 100) %>%
arrange(desc(count))# Arrange the counts in descending order
DemoGraphicsData
#plot graph
DemoGraphicsData[] %>%
arrange(count) %>%
mutate(Country=factor(Country, levels=Country)) %>%
ggplot( aes(x=Country, y=count)) +
geom_segment( aes(xend=Country, yend=0)) +
geom_point( size=2, color="orange") +
coord_flip() + labs(title="Participates by Country")+
theme_minimal()
# to findout most recommend language
First_language <- clean_rawData %>%
# Remove any rows where the respondent didn't answer the question
filter(trimws(clean_rawData$`First language`)!='')%>%
# Group by the responses to the question
group_by(`First language`) %>%
# Count how many respondents selected each option
summarise(count = n()) %>%
# Calculate what percent of respondents selected each option
mutate(percent = (count / sum(count)) * 100) %>%
# Arrange the counts in descending order
arrange(desc(count))
First_language
# to findout have to learn language
Neccecery_language <- clean_rawData %>%
# Remove any rows where the respondent didn't answer the question
filter(trimws(clean_rawData$`must language`)!='')%>%
# Group by the responses to the question
group_by(`must language`) %>%
# Count how many respondents selected each option
summarise(count = n()) %>%
# Calculate what percent of respondents selected each option
mutate(percent = (count / sum(count)) * 100) %>%
# Arrange the counts in descending order
arrange(desc(count))
Neccecery_language
clean_rawData_helpful <- clean_rawData %>%
select(Blogs:Textbooks)
clean_rawData_helpful
#clean record
clean_rawData_helpful<- na.omit(clean_rawData_helpful)
clean_rawData_helpful[] <- lapply(clean_rawData_helpful, factor)
clean_rawData_helpful
# helpfull the plot
helpful_chart <- likert(clean_rawData_helpful)
plot(helpful_chart)
df.v<- summary(helpful_chart) #for references in result
Techdata <- clean_rawData %>%
select(`Amazon Machine Learning`:Tableau)
Techdata<- na.omit(Techdata)
Techdata
Techdata[ ,1:18]  <- lapply(Techdata[ ,1:18],
FUN = function(x) Recode(x, "'Nice to have' =1 ;'Necessary'=2 ;'Unnecessary' = 0"))
Techdata
temp1 <- names(Techdata)
temp2<- list(colSums(Techdata))
temp2<- data.frame(matrix(unlist(colSums(Techdata)), byrow=T))
scale <- cbind(temp1,temp2)
names(scale)[1] <-"Tech_Skill"
names(scale)[2] <-"Rank"
scale <- scale %>%
arrange(desc(Rank))
scale
#plot graph
scale %>%
mutate(Tech_Skill = fct_reorder(Tech_Skill, Rank)) %>%
ggplot( aes(x=Tech_Skill, y=Rank)) +
geom_segment( aes(xend=Tech_Skill, yend=1)) +
geom_point( size=2, color="orange") +
coord_flip() + labs(title="Technical Skills Survey")+
theme_minimal()
softdata <- clean_rawData %>%
select(`Intellectual curiosity`:`Generating Hypotheses`)
softdata<- na.omit(softdata)
softdata
temp3 <- names(softdata)
temp4<- data.frame(matrix(unlist(colSums(softdata)), byrow=T))
scale1 <- cbind(temp3,temp4)
names(scale1)[1] <-"Soft_skill"
names(scale1)[2] <-"Total"
scale1 <- scale1 %>%
arrange(desc(Total))
scale1
#plot graph
scale1 %>%
mutate(Soft_skill = fct_reorder(Soft_skill, Total)) %>%
ggplot( aes(x=Soft_skill, y=Total)) +
geom_segment( aes(xend=Soft_skill, yend=1)) +
geom_point( size=2, color="orange") +
coord_flip() + labs(title="Soft Skills Survey")+
theme_minimal()
knitr::opts_chunk$set(echo = TRUE)
if (!require('jsonlite')) install.packages('jsonlite')
if (!require('dplyr')) install.packages('dplyr')
if (!require('stringr')) install.packages('stringr')
# get our API key
source("configAPI.R")
if (!require('jsonlite')) install.packages('jsonlite')
if (!require('dplyr')) install.packages('dplyr')
if (!require('stringr')) install.packages('stringr')
install.packages("devtools")
devtools::install_github("mkearney/nytimes")
install.packages("devtools")
knitr::opts_chunk$set(echo = TRUE)
# get our API key
source("configAPI.R")
# get our API key
# add text required for call
addkey <- paste0("a831fcb0f92846d6868c57b9cea31d22", key)
# add url and text required for call
url <- "https://api.nytimes.com/svc/topstories/v1/technology.json"
addurl <- paste0(url, "?")
# make call
feed <- fromJSON(paste0(addurl, a831fcb0f92846d6868c57b9cea31d22))
knitr::opts_chunk$set(echo = TRUE)
if (!require('jsonlite')) install.packages('jsonlite')
if (!require('dplyr')) install.packages('dplyr')
if (!require('stringr')) install.packages('stringr')
if (!require('devtools')) install.packages('devtools')
# get our API key
key <- a831fcb0f92846d6868c57b9cea31d22
# get our API key
key <- "a831fcb0f92846d6868c57b9cea31d22"
# add text required for call
# add text required for call
addkey <- paste0("&api-key=", key)
# add url and text required for call
url <- "https://api.nytimes.com/svc/topstories/v1/technology.json"
addurl <- paste0(url, "?")
# make call
feed <- fromJSON(paste0(addurl, addkey))
sub <- feed$results
sub$pub_date <- unlist(str_extract(sub$published_date, ".+?(?=T)"))
sub$pub_time <- unlist(str_extract(sub$published_date, "(?<=T)(.*)"))
sub <- sub %>%
filter(section == "Technology") %>%
select(title, abstract, byline, url, pub_date, pub_time)
knitr::kable(sub)
# add url and text required for call
url <- "https://api.nytimes.com/lists/best-sellers/history.json"
addurl <- paste0(url, "?")
# make call
feed <- fromJSON(paste0(addurl, addkey))
# add url and text required for call
url <- "https://api.nytimes.com/svc/books/v3/lists/best-sellers/history.json"
addurl <- paste0(url, "?")
# make call
feed <- fromJSON(paste0(addurl, addkey))
sub <- feed$results
sub$pub_date <- unlist(str_extract(sub$published_date, ".+?(?=T)"))
sub <- feed$results
sub
# add url and text required for call
url <- "https://api.nytimes.com/svc/books/v3/lists//.json"
addurl <- paste0(url, "?")
# make call
feed <- fromJSON(paste0(addurl, addkey))
# add url and text required for call
url <- "https://api.nytimes.com/svc/books/v3/lists.json"
addurl <- paste0(url, "?")
# make call
feed <- fromJSON(paste0(addurl, addkey))
# add url and text required for call
url <- "https://api.nytimes.com/svc/mostpopular/v2/mostshared/Technology/.json"
addurl <- paste0(url, "?")
# make call
feed <- fromJSON(paste0(addurl, addkey))
knitr::opts_chunk$set(echo = TRUE)
if (!require('jsonlite')) install.packages('jsonlite')
if (!require('dplyr')) install.packages('dplyr')
if (!require('stringr')) install.packages('stringr')
if (!require('devtools')) install.packages('devtools')
# get our API key
key <- "a831fcb0f92846d6868c57b9cea31d22"
# add text required for call
# add text required for call
addkey <- paste0("&api-key=", key)
# add url and text required for call
url <- "https://api.nytimes.com/svc/mostpopular/v2/mostshared/Technology/.json"
addurl <- paste0(url, "?")
# make call
feed <- fromJSON(paste0(addurl, addkey))
sub <- feed$results
knitr::opts_chunk$set(echo = TRUE)
if (!require('jsonlite')) install.packages('jsonlite')
if (!require('dplyr')) install.packages('dplyr')
if (!require('stringr')) install.packages('stringr')
# get our API key
source("configAPI.R")
# get our API key
source("configAPI.R")
# add text required for call
addkey <- paste0("&api-key=", key)
# add url and text required for call
url <- "https://api.nytimes.com/svc/mostpopular/v2/mostshared/Technology/.json"
addurl <- paste0(url, "?")
# make call
feed <- fromJSON(paste0(addurl, addkey))
knitr::opts_chunk$set(echo = TRUE)
if (!require('jsonlite')) install.packages('jsonlite')
if (!require('dplyr')) install.packages('dplyr')
if (!require('stringr')) install.packages('stringr')
if (!require('devtools')) install.packages('devtools')
# get our API key
key <- "a831fcb0f92846d6868c57b9cea31d22"
# add text required for call
# add text required for call
addkey <- paste0("&api-key=", key)
# add url and text required for call
url <- "https://api.nytimes.com/svc/books/v3/lists/best-sellers/history.json"
addurl <- paste0(url, "?")
# make call
feed <- fromJSON(paste0(addurl, addkey))
sub <- feed$results
sub
sub$pub_date <- unlist(str_extract(sub$published_date, ".+?(?=T)"))
sub <- feed$results
sub
sub$pub_date <- unlist(str_extract(sub$published_date, ".+?(?=T)"))
sub <- feed$results
sub
# add url and text required for call
url <- "https://api.nytimes.com/lists/best-sellers/history.json"
addurl <- paste0(url, "?")
# make call
feed <- fromJSON(paste0(addurl, addkey))
knitr::opts_chunk$set(echo = TRUE)
if (!require('jsonlite')) install.packages('jsonlite')
if (!require('dplyr')) install.packages('dplyr')
if (!require('stringr')) install.packages('stringr')
if (!require('devtools')) install.packages('devtools')
# get our API key
key <- "a831fcb0f92846d6868c57b9cea31d22"
# add text required for call
# add text required for call
addkey <- paste0("&api-key=", key)
knitr::opts_chunk$set(echo = TRUE)
if (!require('jsonlite')) install.packages('jsonlite')
if (!require('dplyr')) install.packages('dplyr')
if (!require('stringr')) install.packages('stringr')
if (!require('devtools')) install.packages('devtools')
# get our API key
key <- "a831fcb0f92846d6868c57b9cea31d22"
# add text required for call
# add text required for call
addkey <- paste0("&api-key=", key)
knitr::opts_chunk$set(echo = TRUE)
if (!require('jsonlite')) install.packages('jsonlite')
if (!require('dplyr')) install.packages('dplyr')
if (!require('stringr')) install.packages('stringr')
if (!require('devtools')) install.packages('devtools')
# get our API key
key <- "a831fcb0f92846d6868c57b9cea31d22"
# add text required for call
# add text required for call
addkey <- paste0("&api-key=", key)
# add url and text required for call
url <- "https://api.nytimes.com/lists/best-sellers/history.json"
addurl <- paste0(url, "?")
# make call
feed <- fromJSON(paste0(addurl, addkey))
knitr::opts_chunk$set(echo = TRUE)
if (!require('jsonlite')) install.packages('jsonlite')
if (!require('dplyr')) install.packages('dplyr')
if (!require('stringr')) install.packages('stringr')
if (!require('devtools')) install.packages('devtools')
# get our API key
key <- "a831fcb0f92846d6868c57b9cea31d22"
# add text required for call
# add text required for call
addkey <- paste0("&api-key=", key)
# add url and text required for call
url <- "https://api.nytimes.com/lists/best-sellers/history.json"
addurl <- paste0(url, "?")
# make call
feed <- fromJSON(paste0(addurl, addkey))
# get our API key
key <- "a831fcb0f92846d6868c57b9cea31d22"
# add text required for call
# add text required for call
addkey <- paste0("&api-key=", key)
# add url and text required for call
url <- "https://api.nytimes.com/svc/topstories/v1/technology.json"
addurl <- paste0(url, "?")
# make call
feed <- fromJSON(paste0(addurl, addkey))
sub <- feed$results
sub
# add url and text required for call
url <- "https://api.nytimes.com/svc/topstories/v2/home.json"
addurl <- paste0(url, "?")
# make call
feed <- fromJSON(paste0(addurl, addkey))
sub <- feed$results
sub
sub <- feed$results
sub
sub$pub_date <- unlist(str_extract(sub$published_date, ".+?(?=T)"))
sub$pub_time <- unlist(str_extract(sub$published_date, "(?<=T)(.*)"))
sub <- sub %>%
select(section, title,byline, abstract,short_url, pub_date, pub_time)
knitr::kable(sub)
# add text required for call
addkey <- paste0("&api-key=", key)
addkey
# add url and text required for call
url <- "https://api.nytimes.com/svc/topstories/v2/home.json"
addurl <- paste0(url, "?")
addurl
# make call
feed <- fromJSON(paste0(addurl, addkey))
feed
addurl
feed
v<-paste0(addurl, addkey)
v
knitr::opts_chunk$set(echo = TRUE)
if (!require('jsonlite')) install.packages('jsonlite')
if (!require('dplyr')) install.packages('dplyr')
if (!require('stringr')) install.packages('stringr')
if (!require('devtools')) install.packages('devtools')
# get our API key
key <- "a831fcb0f92846d6868c57b9cea31d22"
# add text required for call
# add text required for call
addkey <- paste0("api-key=", key)
addkey
knitr::opts_chunk$set(echo = TRUE)
if (!require('jsonlite')) install.packages('jsonlite')
if (!require('dplyr')) install.packages('dplyr')
if (!require('stringr')) install.packages('stringr')
if (!require('devtools')) install.packages('devtools')
# get our API key
key <- "a831fcb0f92846d6868c57b9cea31d22"
# add text required for call
# add text required for call
addkey <- paste0("api-key=", key)
addkey
# add url and text required for call
url <- "https://api.nytimes.com/svc/topstories/v2/home.json"
addurl <- paste0(url, "?")
addurl
v<-paste0(addurl, addkey)
v
# make call
feed <- fromJSON(paste0(addurl, addkey))
feed
sub <- feed$results
sub
sub$pub_date <- unlist(str_extract(sub$published_date, ".+?(?=T)"))
sub$pub_time <- unlist(str_extract(sub$published_date, "(?<=T)(.*)"))
sub <- sub %>%
select(section, title,byline, abstract,short_url, pub_date, pub_time)
knitr::kable(sub)
sub
sub <- feed$results
sub
sub$pub_date <- unlist(str_extract(sub$published_date, ".+?(?=T)"))
sub$pub_time <- unlist(str_extract(sub$published_date, "(?<=T)(.*)"))
sub <- sub %>%
select(section, title,byline, abstract,short_url, pub_date, pub_time)
knitr::opts_chunk$set(echo = TRUE)
if (!require('jsonlite')) install.packages('jsonlite')
if (!require('dplyr')) install.packages('dplyr')
if (!require('stringr')) install.packages('stringr')
if (!require('devtools')) install.packages('devtools')
# get our API key
key <- "api-key=a831fcb0f92846d6868c57b9cea31d22"
# add text required for call
# add url and text required for call
url <- "https://api.nytimes.com/svc/topstories/v2/home.json"
addurl <- paste0(url, "?")
# make call
feed <- fromJSON(paste0(addurl, key))
feed
sub <- feed$results
sub
sub$pub_date <- unlist(str_extract(sub$published_date, ".+?(?=T)"))
sub$pub_time <- unlist(str_extract(sub$published_date, "(?<=T)(.*)"))
sub <- sub %>%
select(section, title,byline, abstract,short_url, pub_date, pub_time)
knitr::kable(sub)
