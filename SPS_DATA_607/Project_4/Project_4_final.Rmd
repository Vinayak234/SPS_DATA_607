---
title: "Untitled"
author: "VINAYAK PATEL"
date: "November 4, 2018"
output:
   html_document:
    collapsed: no
    theme: cosmo
    toc: yes
    toc_float: no
    toc_depth: 2
    df_print: paged
---

# Assignment: Document Classification

It can be useful to be able to classify new "test" documents using already classified "training" documents.  A common example is using a corpus of labeled spam and ham (non-spam) e-mails to predict whether or not a new document is spam.  

For this project, you can start with a spam/ham dataset, then predict the class of new documents (either withheld from the training dataset or from another source such as your own spam folder).   One example corpus:  https://spamassassin.apache.org/publiccorpus/

For more adventurous students, you are welcome (encouraged!) to come up with a different set of documents (including scraped web pages!?) that have already been classified (e.g. tagged), then analyze these documents to predict how new documents should be classified.

* Read in the Data
* Count occurrences of each word (Train)
* Classify each Test Mail (Test)
* Print Results

# **Required Packages**
```{r, warning=FALSE, message=FALSE}
if (!require('tm')) install.packages('tm')
if (!require('stopwords')) install.packages('stopwords')
if (!require('plyr')) install.packages('plyr')
if (!require('knitr')) install.packages('knitr')
if (!require('wordcloud')) install.packages('wordcloud')
if (!require('SnowballC')) install.packages('SnowballC')
library("RColorBrewer")
library(e1071)
library(dplyr)
library('ISOcodes')
```


### Imports:

### Function to Load Corpus:

```{r message=FALSE, warning=FALSE}
# My computer crashes when I go higher than MAX_FILES...
MAX_FILES <- 40
# How many table rows to show for knitr tables
NUM_TABLE_ROWS <- 10
# how many terms to return on analysis
TOP_X_TERMS <- 999
# get the corpus given the directory
get_corpus <- function(the_dir){
  file_contents <- c()
  the_files <- list.files(path=the_dir, full.names = TRUE)
  head(the_files)
  i <- 0
  for (cur_file in the_files){
    if(i < MAX_FILES){
      current_content <- readLines(cur_file)
      file_contents <- c(file_contents, current_content)
      i <- (i+1)
    }
  }
  the_corpus <- Corpus(VectorSource(file_contents))
  return (the_corpus)
}
```

### Create the SPAM/HAM Corpuses:

```{r message=FALSE, warning=FALSE}
# Get the Ham and Spam Corpuses:
ham_corpus <- get_corpus("easy_ham/")
length(ham_corpus)
ham_corpus
spam_corpus <- get_corpus("spam/")
length(spam_corpus)
spam_corpus

cleanfunction<- function(corpus) {
  toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
  tmp <- tm_map(corpus, toSpace, "\\n")
  tmp <- tm_map(tmp, toSpace, "<.*?>")
  tmp <- tm_map(tmp, toSpace, "^.*Subject: ")
  tmp <- tm_map(tmp, content_transformer(tolower))
  tmp <- tm_map(tmp, content_transformer(removePunctuation)) 
  tmp <- tm_map(tmp, content_transformer(removeNumbers))
  tmp <- tm_map(tmp,  removeWords, c(stopwords("english")))
  tmp <- tm_map(tmp, content_transformer(stemDocument))  
  tmp <- tm_map(tmp, content_transformer(stripWhitespace))
}

ham_corpus<-cleanfunction(ham_corpus)


spam_corpus<-cleanfunction(spam_corpus)

```

### Filter the 2 Corpuses, and create Term Document Matrices:

```{r message=FALSE, warning=FALSE}
# general filtering opts:
tdm_dtm_opts <- list(removePunctuation=TRUE, removeNumbers=TRUE, stripWhitespace=TRUE, tolower=TRUE, stopwords=TRUE, minWordLength = 2)

tdm_dtm_opts
# create the TDMs
spam_tdm <- TermDocumentMatrix(spam_corpus,control=tdm_dtm_opts)
spam_tdm
ham_tdm <- TermDocumentMatrix(ham_corpus,control=tdm_dtm_opts)
ham_tdm
spam_corpus
```

### Create Spam and Ham Data Frames:

```{r message=FALSE, warning=FALSE}
spam_df <- as.data.frame(as.table(spam_tdm))
spam_df$spam_ham <- "SPAM"
colnames(spam_df) <- c('TERM', 'SPAM_DOCS', 'SPAM_FREQ', 'TYPE_SPAM')
spam_df <- subset(spam_df, select = -c(2) )
spam_df$SPAM_FREQ[is.na(spam_df$SPAM_FREQ)] <- '0'
spam_df <- ddply(spam_df, .(TERM, TYPE_SPAM), summarize, SPAM_FREQ = sum(as.numeric(SPAM_FREQ)))
kable(head(spam_df, n = NUM_TABLE_ROWS))
spam_count <- nrow(spam_df)
ham_df <- as.data.frame(as.table(ham_tdm))
ham_df$spam_ham <- "HAM"
colnames(ham_df) <- c('TERM', 'HAM_DOCS', 'HAM_FREQ', 'TYPE_HAM')
ham_df <- subset(ham_df, select = -c(2) )
ham_df$HAM_FREQ[is.na(ham_df$HAM_FREQ)] <- '0'
ham_df <- ddply(ham_df, .(TERM, TYPE_HAM), summarize, HAM_FREQ = sum(as.numeric(HAM_FREQ)))
kable(head(ham_df, n = NUM_TABLE_ROWS))
ham_count <- nrow(ham_df)
```

### Merge the Spam and Ham Data Frames:

```{r message=FALSE, warning=FALSE}
# now hopefully merge them with no memory issues..
all_df <- merge(x = ham_df, y = spam_df, by="TERM", all = TRUE)
# since this is like an outer join, fill the nulls with Zeros...
all_df$SPAM_FREQ[is.na(all_df$SPAM_FREQ)] <- '0'
all_df$TYPE_SPAM[is.na(all_df$TYPE_SPAM)] <- 'SPAM'
all_df$HAM_FREQ[is.na(all_df$HAM_FREQ)] <- '0'
all_df$TYPE_HAM[is.na(all_df$TYPE_HAM)] <- 'HAM'
all_df[is.na(all_df)] <- '0'
```  

### Take a look at the SpamHam DataFrame sorted by HAM_FREQ desc, then SPAM_FREQ desc

```{r message=FALSE, warning=FALSE}
all_df$SPAM_WEIGHT <- as.numeric(all_df$SPAM_FREQ) - as.numeric(all_df$HAM_FREQ)
kable(head(all_df[order(-as.numeric(all_df$HAM_FREQ)), ], n=NUM_TABLE_ROWS))
kable(head(all_df[order(-as.numeric(all_df$SPAM_FREQ)), ], n=NUM_TABLE_ROWS))
```  

### HAM CLOUD

```{r message=FALSE, warning=FALSE}
wordcloud(ham_corpus, max.words = 200, random.order = FALSE, colors=c('green'))
```

### SPAM CLOUD

```{r message=FALSE, warning=FALSE}
wordcloud(spam_corpus, max.words = 200, random.order = FALSE, colors=c('red'))
```

### Function to calculate the spam score (positive means more likely to be spam...):
